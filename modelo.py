# -*- coding: utf-8 -*-
"""Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wf2eMp7wMr_UbfMyg6BmeMxUTEdfA1ee

# **Análisis de Similitud de Jugadores de la Premier League Mediante Redes Neuronales con Datos de FBREF**

> **Universidad Nacional Autonoma de México**
>
> **Diplomado de Ciencia de Datos**
>
> **Autor:** [Daniel Alejandro Cervantes huerta](https://github.com/Daxcer/Futbol_Analisis)
>
> **Contacto:** [danacer376@gmail.com](mailto:danacer376@gmail.com)

##Introducción

En el mundo del fútbol inglés, cada fin de semana, las emociones suben al máximo, y los fanáticos de la Premier League disfrutan de una de las competiciones más exigentes y emocionantes del planeta. Pero detrás de cada pase, cada tiro, y cada movimiento en el campo, existe una cantidad de datos fascinantes que pueden decirnos mucho más de lo que vemos a simple vista. Para quienes desean ir más allá de las jugadas y comprender los números y patrones detrás del juego, el análisis de datos se ha convertido en una herramienta increíblemente interesante y accesible.

Este proyecto presenta un modelo de redes neuronales, diseñado para ayudar a los aficionados a descubrir las similitudes entre sus jugadores favoritos de la Premier League. ¿Te has preguntado alguna vez cómo se compara tu delantero favorito con los grandes goleadores de la liga? O, ¿qué tanto se parecen en estilo y rendimiento dos mediocampistas clave en diferentes equipos? Utilizando datos de FBREF, una fuente confiable de estadísticas futbolísticas, este modelo permite explorar y comparar el rendimiento de los jugadores, encontrando patrones y estilos de juego similares entre ellos.

A través de este análisis, los aficionados podrán sumergirse en los datos como nunca antes, desarrollando un entendimiento más profundo del fútbol y experimentando la emoción de analizar a sus ídolos. Con esta herramienta, no solo verás el fútbol como siempre, sino que tendrás la oportunidad de acercarte a él desde una perspectiva analítica y educativa, descubriendo qué hace único a cada jugador y cómo se entrelazan sus habilidades con el equipo y el juego en general.

##Problema que se Desea Resolver

Para quienes siguen semana a semana la Premier League, entender por qué ciertos jugadores destacan en el campo no siempre es sencillo. Aunque los datos estadísticos están disponibles para todos, interpretar la relevancia de esos números y compararlos entre jugadores no es una tarea obvia para el aficionado promedio. A menudo, los seguidores se encuentran con términos técnicos o métricas avanzadas que dificultan sacar conclusiones sobre el rendimiento de jugadores de distintas posiciones, equipos o estilos de juego. Además, muchos se preguntan cómo saber si dos jugadores tienen similitudes en su estilo o en qué aspectos uno podría mejorar para rendir más como otro.

La solución a este problema viene de la mano de un modelo basado en redes neuronales, que permite agrupar a los jugadores según patrones similares en sus estadísticas. Este modelo utiliza información de la base de datos de FBREF para analizar múltiples aspectos del rendimiento de cada jugador, como los pases, la recuperación de balón, los tiros y su influencia en el juego. Al procesar estos datos, el modelo identifica jugadores con patrones estadísticos similares, permitiendo a los aficionados comparar de manera fácil y accesible a sus ídolos del fútbol inglés y descubrir aquellos con estilos de juego parecidos.

**Solución Propuesta**

Este modelo actúa como una especie de “buscador de similitudes”, usando redes neuronales para analizar el rendimiento de los jugadores de la Premier League en función de sus estadísticas individuales. Lo que hace es “aprender” de los datos, detectando patrones y agrupando a los jugadores en base a sus características de juego. La red neuronal es capaz de hacer estas comparaciones de una forma muy precisa, analizando decenas de estadísticas que los aficionados, por sí solos, tendrían dificultades para sintetizar.

Para el usuario, la herramienta será como un portal de descubrimiento: podrán seleccionar un jugador y encontrar a otros con estilos de juego y métricas similares. Así, este modelo no solo ayuda a entender las estadísticas de manera divertida y educativa, sino que permite a los fanáticos hacer comparaciones directas y plantearse preguntas interesantes, como cuál es el mediocampista más completo o qué delantero tiene una capacidad de presión similar a otro.

En resumen, el modelo ofrece una solución sencilla y visual para que cualquier aficionado pueda explorar y aprender más sobre el fútbol a través de los datos, rompiendo la barrera técnica y acercando la ciencia del deporte a los apasionados del juego.

##Tecnologías Utilizadas

Para abordar la complejidad de este análisis, se emplean diversas tecnologías:

1. **Python**: Lenguaje de programación base para el análisis y modelado.

2. **Selenium**: Herramienta para el web scraping, usada para la extracción de datos de FBREF debido a su capacidad para manejar datos dinámicos.

3. **SQLite**: Base de datos relacional utilizada inicialmente para almacenar los datos, facilitando consultas rápidas y almacenaje de gran volumen de información sin cargar en memoria.

4. **Pandas**: Utilizado para el análisis y manipulación de datos, proporcionando herramientas para trabajar con grandes volúmenes de información.

5. **Dask**: Biblioteca de Python que permite trabajar con grandes conjuntos de datos, facilitando la manipulación en paralelo de archivos parquet.

6. **TensorFlow / Keras**: Herramientas de aprendizaje profundo para entrenar el modelo de redes neuronales.

7. **Google Colab**: Se utiliza un notebook en colab para la realización del modelo. Es necesario el uso de una TPU para poder procesar la base de datos por su tamaño considerable.
"""

!pip install selenium
!pip install dask
!pip install tensorflow
!pip install keras

"""##Cronograma del Proyecto

Para asegurar una gestión eficiente del tiempo y la ejecución ordenada de las tareas, se definió un cronograma:

|Fase|Actividades|Duración Estimada|
|----|-----------|-----------------|
|Extracción de Datos|	Web scraping con Selenium en FBREF para la obtención de datos de jugadores en múltiples temporadas.|	Semana 1|
|Almacenamiento y Estructura de Datos|	Almacenamiento en SQLite y consultas exploratorias de nombres de tablas, columnas y tipos de datos.	|Semana 1|
|Transformación y Consolidación|	Creación de tablas de cada temporada, limpieza y consolidación en archivos .parquet.|	Semana 2|
|Análisis Exploratorio	|Análisis de las métricas, visualización de estadísticas y patrones.|	Semana 2|
|Preparación de Datos|	Escalado y preparación para el modelo.|	Semana 2|
|Entrenamiento del Modelo|	Configuración, entrenamiento y ajuste del modelo de similitud basado en redes neuronales.|	Semana 3|
|Evaluación y Ajustes|	Validación del modelo, interpretación de resultados y ajustes necesarios.|	Semana 4|
|Documentación|	Creación de la presentación del proyecto, redacción y compilación de resultados.|	Semana 4|

##Objetivos del Proyecto

1. **Desarrollar un Modelo de Similitud de Jugadores:** Crear un modelo basado en redes neuronales que identifique patrones estadísticos entre jugadores de la Premier League, utilizando datos históricos y actuales de FBREF. Este modelo permitirá a los aficionados encontrar jugadores con estilos y rendimientos similares, ofreciendo una manera accesible y entretenida de explorar las características y comparativas de los futbolistas de la liga.

2. **Implementar una Interfaz Interactiva en Streamlit:** Facilitar la experiencia del usuario al desarrollar una interfaz en Streamlit que permita analizar y visualizar datos de manera intuitiva. La aplicación incluirá herramientas interactivas como un Pizza Chart para cada jugador, ofreciendo una representación visual y detallada de sus métricas clave. Esto ayudará a los usuarios a identificar rápidamente las fortalezas y debilidades de cada jugador en una variedad de aspectos, como pases, defensa, remates, entre otros.

3. **Integrar y Visualizar Datos Históricos y de Equipos:** Proveer un marco de datos en la aplicación con estadísticas de jugadores de cada equipo y temporada de la Premier League. Este DataFrame contendrá información detallada, organizada por equipo y temporada, y será capaz de alimentar el modelo y la interfaz para brindar una visión completa del rendimiento histórico y actual de los jugadores. Esta funcionalidad permitirá a los usuarios explorar la evolución de los jugadores a lo largo de las temporadas y comparar a sus favoritos en diversos periodos.

4. **Proporcionar un Enfoque Educativo y Accesible al Análisis de Datos Deportivos:** Ofrecer a los aficionados una herramienta educativa que les permita aprender sobre análisis de datos aplicados al fútbol de manera sencilla y visual. Con esta herramienta, podrán experimentar el proceso de interpretación y comparación de estadísticas futbolísticas, familiarizándose con conceptos clave en la ciencia del deporte y el análisis de rendimiento, todo sin necesidad de conocimientos técnicos avanzados.

##Descripción del Proceso de Extracción de Datos

Los datos de rendimiento de los jugadores fueron recolectados mediante un proceso de web scraping utilizando Selenium, una herramienta de automatización especialmente útil para interactuar con páginas web dinámicas. En este caso, FBREF fue elegido como fuente por su amplia y detallada base de datos, la cual incluye estadísticas de rendimiento de jugadores en diversas temporadas, abarcando métricas de pases, tiros, defensas, y muchas otras que son fundamentales para un análisis completo.

La razón de emplear Selenium en lugar de herramientas de extracción más simples, como pandas (que funciona bien para páginas estáticas), se debe a la naturaleza dinámica de FBREF. Las tablas en este sitio están en constante cambio y se generan de manera interactiva cada vez que el usuario accede o realiza una consulta en la página. Esto significa que, para poder capturar correctamente la información de estas tablas, es necesario contar con un método que permita “navegar” y actualizar el contenido, como lo haría un usuario humano, y Selenium proporciona esta capacidad. Con Selenium, se programaron pasos para acceder a las páginas específicas de cada jugador y temporada, cargar el contenido dinámico y extraer los datos actualizados de forma precisa, superando las limitaciones de otras herramientas menos interactivas.

**Selección de Temporadas y Tablas:**

Para construir una base de datos sólida y completa, se recopilaron datos de siete temporadas completas de la Premier League. En cada temporada, se seleccionaron 11 tablas estadísticas clave que cubren una variedad de métricas de rendimiento esenciales para evaluar y comparar a los jugadores en diferentes aspectos del juego. Estas tablas abarcan desde las estadísticas estándar hasta métricas avanzadas de rendimiento en áreas específicas, permitiendo un análisis detallado y segmentado de cada jugador.

Las 11 tablas seleccionadas fueron:

- **stats_standard:** Contiene estadísticas generales y básicas de cada jugador, como goles, asistencias, minutos jugados, tarjetas y demás métricas fundamentales.

- **stats_keeper:** Incluye estadísticas específicas para porteros, como atajadas, goles concedidos y arcos en cero, que son métricas clave para evaluar su rendimiento bajo los tres palos.

- **stats_keeper_adv:** Presenta estadísticas avanzadas de porteros, incluyendo detalles sobre el tipo de disparos enfrentados y el éxito en atajadas de alto riesgo.

- **stats_shooting:** Aborda las métricas de tiro, como cantidad de disparos, precisión y goles esperados (xG), proporcionando una visión detallada de la eficacia de los jugadores en el ataque.

- **stats_passing:** Incluye datos sobre precisión de pases, asistencias y tipos de pase, aspectos que son fundamentales para evaluar el rendimiento de mediocampistas y defensores en la distribución del balón.

- **stats_passing_types:** Contiene información adicional sobre diferentes tipos de pases, como pases largos, cortos, progresivos, entre otros, lo cual ayuda a entender el estilo de distribución y toma de decisiones de cada jugador.

- **stats_gca (Goal-Creating Actions):** Se enfoca en las acciones que generan oportunidades de gol, como asistencias clave, pases previos al gol y otras jugadas que contribuyen directamente a la creación de oportunidades de anotación.

- **stats_defense:** Proporciona métricas defensivas, como entradas, intercepciones y despejes, que son fundamentales para analizar el rendimiento de los defensores y la contribución defensiva de otros jugadores.

- **stats_possession:** Abarca estadísticas sobre la posesión del balón, como toques, tiempo de posesión y acciones en áreas clave, lo cual ofrece una perspectiva completa del control de juego de cada jugador.

- **stats_playing_time:** Registra el tiempo de juego, número de partidos y minutos disputados, ayudando a evaluar la consistencia y regularidad de los jugadores en la alineación.

- **stats_misc:** Contiene estadísticas variadas como faltas recibidas y cometidas, duelos ganados y otros datos que aportan información complementaria sobre el desempeño de cada jugador.

Esta selección de tablas permite construir una vista multifacética del rendimiento de cada jugador a lo largo de las temporadas, facilitando el análisis comparativo y la identificación de similitudes entre ellos. La diversidad de métricas incluidas asegura que el modelo pueda captar desde habilidades ofensivas y defensivas hasta patrones de posesión y desempeño específico por posición.

###**Almacenamiento en SQLite**:

Para manejar el volumen de datos extraídos y evitar problemas de memoria, especialmente cuando se trabaja con grandes cantidades de información histórica y estadística de jugadores, se decidió almacenar los datos en una base de datos SQLite, llamada data.db. SQLite es una base de datos ligera y eficiente que permite guardar y gestionar datos localmente sin requerir configuraciones de servidor, lo que la convierte en una opción ideal para proyectos de análisis de datos personales o educativos.

Con SQLite, se pueden realizar consultas SQL sobre la información almacenada, permitiendo una rápida y flexible recuperación de datos cuando sea necesario, sin necesidad de volver a realizar el proceso de extracción. Esto significa que el usuario puede filtrar, ordenar y comparar estadísticas específicas —como datos por posición, temporada o equipo— sin ralentizar el sistema. Además, esta organización en una base de datos relacional facilita su conversión sin saturar la memoria RAM del entorno, centrandose en la memoria de Disco, optimizando el rendimiento y reduciendo el uso de recursos en el dispositivo.
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import pandas as pd
import sqlite3

# Iniciamos el WebDriver
options = webdriver.ChromeOptions()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')
driver = webdriver.Chrome(options=options)

# Conectamos a la base de datos SQLite (Si no existe se crea una)
conn = sqlite3.connect('/content/data.db')

# Lista de temporadas a considerar (Son todas las que estan disponibles en FBREF)
seasons = ['2024-2025', '2023-2024', '2022-2023', '2021-2022', '2020-2021', '2019-2020', '2018-2019']

# Se crea un diccionario con el ID de la tabla y su URL
urls = {
    'stats_standard': 'https://fbref.com/en/comps/9/{}/stats/Premier-League-Stats',
    'stats_keeper': 'https://fbref.com/en/comps/9/{}/keepers/Premier-League-Stats',
    'stats_keeper_adv': 'https://fbref.com/en/comps/9/{}/keepersadv/Premier-League-Stats',
    'stats_shooting': 'https://fbref.com/en/comps/9/{}/shooting/Premier-League-Stats',
    'stats_passing': 'https://fbref.com/en/comps/9/{}/passing/Premier-League-Stats',
    'stats_passing_types': 'https://fbref.com/en/comps/9/{}/passing_types/Premier-League-Stats',
    'stats_gca': 'https://fbref.com/en/comps/9/{}/gca/Premier-League-Stats',
    'stats_defense': 'https://fbref.com/en/comps/9/{}/defense/Premier-League-Stats',
    'stats_possession': 'https://fbref.com/en/comps/9/{}/possession/Premier-League-Stats',
    'stats_playing_time': 'https://fbref.com/en/comps/9/{}/playingtime/Premier-League-Stats',
    'stats_misc': 'https://fbref.com/en/comps/9/{}/misc/Premier-League-Stats'
}

# Definimos la función para cargar la tabla toamando el ID y el URL
def load_table(url, table_id):
  driver.get(url)
  WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, table_id)))
  soup = BeautifulSoup(driver.page_source, 'html.parser')
  table = soup.find(id=table_id)

  # Ajustamos los MultiIndex en los nombres de la columnas y convertimos los valores a numéricos
  if table:
    df = pd.read_html(str(table))[0]

    if isinstance(df.columns, pd.MultiIndex):
      df.columns = ['.'.join(filter(None, col)).strip() for col in df.columns]

    # Definir las columnas que deben ser texto
    text_columns = [
            'Unnamed: 0_level_0.Rk', 'Unnamed: 1_level_0.Player',
            'Unnamed: 2_level_0.Nation', 'Unnamed: 3_level_0.Pos',
            'Unnamed: 4_level_0.Squad', 'Unnamed: 5_level_0.Age',
            'Unnamed: 6_level_0.Born'
        ]

    # Renombrar las columnas eliminando el prefijo 'Unnamed: ...'
    new_column_names = {
            'Unnamed: 0_level_0.Rk': 'Rk',
            'Unnamed: 1_level_0.Player': 'Player',
            'Unnamed: 2_level_0.Nation': 'Nation',
            'Unnamed: 3_level_0.Pos': 'Pos',
            'Unnamed: 4_level_0.Squad': 'Squad',
            'Unnamed: 5_level_0.Age': 'Age',
            'Unnamed: 6_level_0.Born': 'Born'
        }
    df.rename(columns=new_column_names, inplace=True)

    for column in df.columns:
      if column not in new_column_names.values():
        df[column] = pd.to_numeric(df[column], errors='coerce')

    # Eliminar la última columna si contiene 'Matches' en su nombre
    last_column = df.columns[-1]
    if 'Matches' in last_column:
        df.drop(columns=[last_column], inplace=True)

    print(f'Columnas en {table_id}: {df.columns.tolist()}')
    return df

  else:
    print(f'No se encontró la tabla con el ID {table_id}')
    return None

# Iteramos sobre las temporadas
for season in seasons:
  for stat_name, stat_url in urls.items():
    formatted_url = stat_url.format(season)
    table_id = stat_name
    df = load_table(formatted_url, table_id)

    if df is not None:
      table_name = f'{season}_{table_id}'
      df.to_sql(table_name, conn, if_exists='replace', index=False)
      print(f'Datos de {table_name} guardados en la base de datos.')
    else:
      print(f'No se pudo obtener la tabla para {table_name}.')

# Cerramos la conección a la base de datos y el WebDriver
conn.close()
driver.quit()
print("Proceso completado, datos guardados en data.db")

"""###**Funciones de Consulta**

Para facilitar el acceso y la interacción con la base de datos data.db y optimizar el análisis de los datos extraídos de FBREF, se definieron diferentes funciones de consulta en el proyecto. Estas funciones permiten al usuario explorar y entender el contenido de la base de datos de forma rápida y organizada, accediendo a nombres de tablas, columnas, tipos de datos y registros específicos. A continuación, se detallan algunas de las principales funciones de consulta:

**Función para Listar Tablas en la Base de Datos**

Esta función permite obtener un listado de todas las tablas disponibles dentro de data.db. Al ejecutarla, el usuario puede ver todas las tablas correspondientes a las distintas métricas y temporadas, facilitando la navegación entre las distintas fuentes de datos almacenadas.

**Función para Listar Columnas de una Tabla Específica**

Dada una tabla en particular (como stats_standard o stats_passing), esta función proporciona un listado de todas las columnas dentro de esa tabla. Esto es especialmente útil para entender las métricas específicas almacenadas y saber qué tipo de información se puede consultar en cada tabla.

**Función para Visualizar el Tipo de Datos de Cada Columna**

Para un análisis más detallado, esta función permite ver el tipo de dato de cada columna en una tabla dada. Dado que data.db almacena diversas métricas, algunas columnas pueden contener datos numéricos (como goles o pases), mientras que otras pueden tener datos de texto (como nombres de jugadores o equipos). Esta función ayuda a identificar la estructura de los datos y permite anticiparse a posibles transformaciones necesarias para el análisis.

**Función para Consultar Datos Específicos de una Tabla**

Esta función permite realizar consultas específicas en una tabla de data.db, eligiendo el límite de datos a ver de la columna seleccionada. De esta forma, el usuario puede visualizar registros concretos sin necesidad de extraer toda la tabla, lo que agiliza la visualización y exploración de los datos.

**Función para conectar con la base de datos**

Esto permite un facíl acceso reduciendo el código y simplificando el proceso.

Estas funciones de consulta están diseñadas para mejorar la accesibilidad a los datos en data.db y permiten al usuario manipular, filtrar y analizar información de manera efectiva sin recurrir a comandos SQL externos. Esto convierte a la base de datos en un recurso dinámico y adaptable para la exploración y visualización de métricas de rendimiento de los jugadores.
"""

import pandas as pd
import sqlite3

# Hacemos la conexión con la base de datos
def connect_db(db_name):
  return sqlite3.connect(db_name)

# Consultamos los nombres de las tablas
def list_tables(conn):
  cursor = conn.cursor()
  cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
  tables = cursor.fetchall()
  return [table[0] for table in tables]

# Consultamos los nombres de las columnas de una tabla a elección
def list_columns(conn, table_name):
  cursor = conn.cursor()
  cursor.execute(f'PRAGMA table_info("{table_name}");')
  columns = cursor.fetchall()
  return [column[1] for column in columns]

# Consultamos los primeros 10 datos de una columna
def get_data(conn, table_name, column_name, limit=10):
  cursor = conn.cursor()
  cursor.execute(f'SELECT "{column_name}" FROM "{table_name}" LIMIT {limit};')
  data = cursor.fetchall()
  return [row[0] for row in data]

# Consultamos información de los datos
def get_data_info(conn, table_name):
  cursor = conn.cursor()

  # Obtener los nombres de las columnas y sus tipos de datos
  cursor.execute(f'PRAGMA table_info("{table_name}");')
  column_info = cursor.fetchall()
  column_names = [row[1] for row in column_info]
  column_types = [row[2] for row in column_info]

  # Contar el número de filas
  cursor.execute(f'SELECT COUNT(*) FROM "{table_name}";')
  num_rows = cursor.fetchall()

  # Imprimimos la información
  print(f'\nInformación de la tabla {table_name}')
  print('Número de filas:', num_rows)
  print('Columnas:')
  for name, type in zip(column_names, column_types):
    print(f'  {name} ({type})')

"""**Nombres de las tablas en data.db**"""

# Mostramos los nombres de las tablas
if __name__ == '__main__':
  conn = connect_db('/content/model_data.db')
  tables = list_tables(conn)
  for table in tables:
    print(table)

"""**Nombres de las columnas en la tabla seleccionada**"""

# Mostramos los nombres de las columnas de la tabla elegida
conn = connect_db('/content/model_data.db')
table_name = '2018-2019_stats_passing' # Poner el nombre de la tabla
columns = list_columns(conn, table_name)
print(f"\nColumnas de la tabla {table_name}:")
for column in columns:
  print(f'"{column}",')

"""**Primeros 10 datos de la columna seleccionada**"""

# Mostramos los primeros 10 datos de una columna
conn = connect_db('/content/model_data.db')
column_name = "defense_Tkl" # Poner el nombre de la columna
data = get_data(conn, table_name, column_name, limit=10)
print(f"\nPrimeras 10 filas de la columna {column_name}:")
for row in data:
  print(row)

"""**Nombres de las columnas en la tabla seleccionada y los tipos de datos en esa columna**"""

# Mostramos información de los datos
conn = connect_db('/content/model_data.db')
data_info = get_data_info(conn, table_name)
print(data_info)
conn.close()

"""###**Creación de Copia de data.db**

Para asegurar la integridad de los datos y permitir pruebas sin riesgo de pérdida de información, se creó una copia de la base de datos original data.db antes de realizar cualquier modificación o experimento. Este duplicado actúa como un entorno de prueba seguro, permitiendo realizar cambios, ajustes y transformaciones de los datos sin comprometer el archivo principal.

El proceso de duplicación se realiza cada vez que es necesario implementar pruebas o realizar cambios sustanciales en la estructura o el contenido de la base de datos. Esto incluye tareas como:

- **Pruebas de Consultas SQL Complejas:** Algunas consultas o uniones de tablas pueden alterar temporalmente la disposición de los datos. Tener una copia permite probar y ajustar estas consultas sin riesgo de dañar los datos originales.

- **Aplicación de Transformaciones y Limpieza de Datos:** Ciertos procesos de limpieza o formateo, como el manejo de valores nulos o la transformación de columnas, requieren validación previa. Trabajar en la copia asegura que, si algo sale mal, el archivo original permanece intacto.

- **Pruebas de Nuevas Funciones de Consulta y Cálculo:** Al desarrollar y ajustar funciones de consulta, es fundamental evaluar el impacto de estas funciones en una copia para verificar su precisión y evitar errores en la base de datos principal.

- **Reinicio Rápido de la Base de Datos:** En casos donde se requiera volver al estado inicial, la copia original permite descartar cambios no deseados y restablecer rápidamente el estado original, sin necesidad de realizar extracciones de datos adicionales.

Esta estrategia de duplicación se implementó de forma sistemática durante el desarrollo, permitiendo la experimentación y asegurando que la base de datos original siempre estuviera disponible y libre de alteraciones. Al preservar data.db sin cambios hasta que los resultados de prueba sean satisfactorios, se garantizó un flujo de trabajo más seguro y eficiente.
"""

import shutil

# Creamos una copia de data.db llamada model_data.db
shutil.copy('data.db', 'model_data.db')
print("Archivo duplicado exitosamente. Ahora puedes trabajar en model_data.db.")

"""###**Cambio de Nombres de las Columnas**

Para lograr una mayor consistencia y orden en la estructura de los datos, se realizó una modificación sistemática en los nombres de las columnas de todas las tablas en la base de datos model_data.db. Este proceso consistió en simplificar y estandarizar los nombres de las columnas, manteniendo únicamente el nombre que aparece antes del guion bajo (_). Esto permitió eliminar redundancias y facilitar la lectura e interpretación de los nombres de las columnas y de las tablas.

**Proceso de Cambio**

Identificación de Componentes Clave: Dado que muchas columnas en los datos extraídos de FBREF incluían nombres largos y a menudo repetitivos (por ejemplo, Performance_Att), se decidió conservar únicamente el término que aparece a la derecha del guion bajo, que suele ser el descriptor principal de la métrica (por ejemplo, Att en lugar de Performance_Att).

**Estándar de Nombres:**

Este proceso no solo facilita la lectura de las tablas, sino que también permite que las columnas de diferentes tablas mantengan nombres consistentes. Por ejemplo, en lugar de tener columnas llamadas Expected_xG en algunas tablas y Performance_xG en otras, ahora se unifican bajo nombres más simples y estandarizados como xG.

**Organización y Accesibilidad:**

Al mantener una convención clara y ordenada en los nombres de las columnas, es más sencillo escribir consultas SQL y hacer referencias cruzadas entre distintas tablas. Esta estandarización también minimiza errores durante el análisis y evita posibles confusiones que podrían surgir de nombres de columnas demasiado específicos o redundantes.

**Ajuste en el Modelo y Visualizaciones:**

Los cambios en los nombres de las columnas se reflejan en el modelo de redes neuronales y en cualquier herramienta de visualización asociada, como el Streamlit y el Pizza Chart. Esto garantiza que tanto el código del modelo como las visualizaciones sean congruentes y fáciles de mantener a lo largo del tiempo.

Esta unificación de los nombres no solo mejora la limpieza y simplicidad de la base de datos, sino que también garantiza una experiencia de consulta más intuitiva y eficiente.
"""

# Conectar a la base de datos
conn = connect_db('/content/model_data.db')
cursor = conn.cursor()

# Columnas comunes que no serán renombradas
common_columns = ['Rk', 'Player', 'Nation', 'Pos', 'Squad', 'Age', 'Born']

# Temporada y nombres de tablas con prefijo de temporada
season = "2018-2019" # Cambiar la temporada
tables = ["stats_standard", "stats_keeper", "stats_keeper_adv", "stats_shooting", "stats_passing",
          "stats_passing_types", "stats_gca", "stats_defense",
          "stats_possession", "stats_playing_time", "stats_misc"]

# Función para renombrar columnas con prefijos
def get_prefixed_column_name(table, column):
    table_abbr = table.split('_')[-1]  # Obtener el prefijo
    column = column.split('_')[-1]     # Usar solo parte después del guión bajo
    if column not in common_columns:
        return f"{table_abbr}_{column}"  # Añadir un guión bajo para evitar confusiones
    return column

# Renombrar columnas y hacerlas persistentes en SQLite
for table in tables:
    full_table_name = f"{season}_{table}"

    # Obtener nombres de columnas y crear lista de columnas renombradas
    cursor.execute(f'PRAGMA table_info("{full_table_name}")')
    columns_info = cursor.fetchall()
    new_columns = [get_prefixed_column_name(table, col[1]) for col in columns_info]

    # Crear nueva tabla con columnas renombradas
    renaming_query = f'CREATE TABLE "{full_table_name}_renamed" AS SELECT '
    renaming_query += ", ".join([f'"{col[1]}" AS "{new_col}"' for col, new_col in zip(columns_info, new_columns)])
    renaming_query += f' FROM "{full_table_name}"'

    # Ejecutar consulta para crear la tabla con columnas renombradas
    cursor.execute(renaming_query)

    # Eliminar la tabla original y renombrar la nueva tabla
    cursor.execute(f'DROP TABLE "{full_table_name}"')
    cursor.execute(f'ALTER TABLE "{full_table_name}_renamed" RENAME TO "{full_table_name}"')
    print(f"Tabla {full_table_name} renombrada y guardada con éxito.")

# Confirmar cambios y cerrar conexión
conn.commit()
conn.close()
print("Renombrado de columnas completado y guardado en la base de datos.")

"""###**Archivos .parquet**

Dada la gran cantidad de datos almacenados en model_data.db, se decidió utilizar el formato .parquet para optimizar el procesamiento y reducir la carga en la memoria RAM al cargar los datos en un DataFrame de pandas. Este formato de archivo es altamente eficiente para almacenar y leer grandes volúmenes de datos en comparación con archivos tradicionales como .csv o .xlsx, gracias a su capacidad de compresión y lectura en bloques.

**Razones para Utilizar .parquet**

- **Eficiencia en el Almacenamiento:** Los archivos .parquet permiten almacenar datos en un formato columnar, lo que facilita una mayor compresión y, por lo tanto, una reducción en el tamaño del archivo. Esto es especialmente útil cuando se manejan múltiples temporadas y tablas con cientos de miles de filas de datos estadísticos de jugadores.

- **Optimización de la Memoria RAM:** Al trabajar con grandes volúmenes de datos en pandas, la conversión directa de model_data.db a un DataFrame de pandas requería una cantidad significativa de memoria RAM, lo que ocasionaba saturación y ralentización del sistema. El formato .parquet, al permitir la lectura de datos en bloques, reduce drásticamente el uso de memoria, permitiendo cargar solo las columnas o registros necesarios.

- **Velocidad de Lectura y Escritura:** En comparación con otros formatos, .parquet ofrece una mayor velocidad de lectura y escritura, lo cual es esencial cuando se realizan múltiples consultas y manipulaciones de datos durante el análisis. Esto permite realizar iteraciones y pruebas de manera más fluida sin que el sistema se vea afectado por demoras o caídas.

- **Compatibilidad con pandas:** El formato .parquet es totalmente compatible con pandas, facilitando el acceso a los datos y permitiendo cargar solo las porciones requeridas para un análisis específico. Esto significa que se pueden realizar cargas parciales, filtrando por temporada o métrica, sin necesidad de cargar la totalidad del archivo en memoria.

**Implementación**

Durante el proceso, cada temporada extraída de model_data.db se convirtió y guardó en un archivo .parquet individual. Esto permitió un manejo más granular de los datos, posibilitando que se carguen en pandas solo cuando sean estrictamente necesarios para los cálculos o visualizaciones. Además, se facilita el procesamiento en paralelo si se desean realizar múltiples análisis a partir de distintas tablas o temporadas.

El uso de .parquet permitió así una gestión eficiente de los recursos y una experiencia de trabajo más ágil y optimizada, ideal para manejar datos de gran volumen en el análisis de rendimiento de los jugadores de la Premier League.

**Elección de Columnas**

Para la elaboración del modelo y la construcción de visualizaciones, se seleccionaron las columnas más representativas de entre las métricas disponibles en **model_data.db**. Estas columnas fueron escogidas cuidadosamente para capturar una variedad de habilidades y contribuciones de los jugadores en diferentes áreas del juego, tales como ataque, defensa y posesión, proporcionando una visión integral del rendimiento en el campo.

**Columnas Seleccionadas y su Justificación**

1. **Player**: Representa el nombre del jugador. Esta columna es fundamental para identificar a cada jugador de forma única en el modelo y en las visualizaciones.

2. **Pos (Position)**: La posición del jugador (por ejemplo, delantero, mediocampista, defensa), que ayuda a contextualizar las estadísticas de rendimiento según el rol específico de cada jugador en el equipo.

3. **Age**: La edad del jugador, un factor relevante en el rendimiento y la evolución de habilidades a lo largo de las temporadas. Es útil para analizar tendencias de rendimiento según la edad y posibles comparaciones entre jugadores jóvenes y veteranos.

4. **standard_Min**: El número de minutos jugados. Este dato es crucial para normalizar estadísticas, ya que permite comparar a jugadores en igualdad de condiciones, tomando en cuenta su tiempo efectivo en el campo.

5. **standard_Gls (Goals)**: La cantidad de goles anotados, que es una de las métricas ofensivas más importantes, especialmente para delanteros y jugadores de ataque.

6. **standard_Ast (Assists)**: El número de asistencias, otra métrica de valor ofensivo que indica la capacidad de un jugador para contribuir al juego colectivo y generar oportunidades de gol.

7. **shooting_Sh (Shots)**: El número total de tiros realizados, que refleja la agresividad ofensiva y la disposición de un jugador para intentar oportunidades de gol.

8. **shooting_SoT (Shots on Target)**: Los tiros al arco o tiros precisos, una medida de la efectividad en la precisión de los disparos.

9. **passing_Cmp (Completed Passes)**: El número de pases completados, una métrica clave para analizar la precisión y habilidad en el manejo de la pelota, especialmente relevante para mediocampistas y jugadores de transición.

10. **passing_Att (Passes Attempted)**: La cantidad de pases intentados, que permite calcular tasas de precisión en el pase y analizar la propensión de un jugador a asumir riesgos en sus decisiones de pase.

11. **defense_Tkl (Tackles)**: La cantidad de entradas realizadas por el jugador, una estadística importante para evaluar la contribución defensiva.

12. **defense_TklW (Tackles Won)**: La cantidad de entradas efectivas, proporcionando una visión de la eficiencia en defensa y la capacidad para recuperar el balón sin cometer faltas.

13. **possession_Touches (Touches)**: El número de veces que el jugador toca el balón, una métrica de posesión que refleja su participación general en el juego.

14. **Season**: Cada fila de datos se complementa con la columna `Season`, que indica la temporada a la que pertenece cada registro de estadísticas. Esto permite el análisis comparativo a lo largo de varias temporadas y la evaluación de la consistencia y evolución del rendimiento de los jugadores.

**Relevancia para el Modelo**

La elección de estas columnas proporciona una representación equilibrada del rendimiento de un jugador en distintos aspectos del juego, desde la ofensiva hasta la defensa y la posesión. Estas estadísticas no solo son útiles para el modelo de redes neuronales que se entrenará para analizar la similitud entre jugadores, sino que también brindan una base sólida para las visualizaciones en **Streamlit**. El **Pizza Chart** desarrollado con estas columnas permite a los usuarios observar y comparar el rendimiento de los jugadores de manera gráfica e intuitiva, resumiendo en una sola visualización varias dimensiones clave del juego.
"""

import sqlite3
import pandas as pd

# Conectar a la base de datos
conn = connect_db('/content/model_data.db')

# Lista de temporadas
seasons = ["2023-2024", "2022-2023", "2021-2022", "2020-2021", "2019-2020", "2018-2019"]

for season in seasons:
    table_name = f"{season}_PL"
    query = f"""
    SELECT "Player", "Pos", "Age", "standard_Min", "standard_Gls", "standard_Ast",
           "shooting_Sh", "shooting_SoT", "passing_Cmp", "passing_Att", "defense_Tkl",
           "defense_TklW", "possession_Touches", '{season}' AS "Season"
    FROM "{table_name}"
    """
    # Leer cada temporada en un DataFrame
    df_season = pd.read_sql_query(query, conn)

    # Guardar en archivo Parquet
    df_season.to_parquet(f"{season}_data.parquet", index=False)
    print(f"Datos de {season} guardados en formato Parquet.")

# Cerrar la conexión
conn.close()

"""###Transformación de los Datos

Para preparar los datos antes de entrenar el modelo de redes neuronales, se realizaron diversas transformaciones en las columnas de tipo texto y numéricas para optimizar la estructura del dataset, facilitando su procesamiento y análisis. Estas transformaciones buscan reducir la complejidad y mejorar la eficiencia del modelo al evitar redundancias y formatos innecesarios.

**Transformaciones Realizadas**

- **Eliminación de la Columna "Squad":** La columna "Squad", que identifica el equipo de cada jugador, fue removida del dataset debido a que su codificación mediante One-Hot Encoding habría generado 29 columnas adicionales, ya que cada equipo de la Premier League estaría representado por una columna única.
Esto hubiese aumentado significativamente la dimensionalidad del dataset, dificultando su procesamiento sin aportar valor significativo al análisis de similitud entre jugadores.

- **Transformación de "Player":** Para hacer que cada jugador sea fácilmente identificable junto con su temporada correspondiente, se combinó el nombre del jugador con el año de la temporada. Por ejemplo, el jugador Marcus Rashford en la temporada 2024-2025 se registró como "2024-2025, Marcus Rashford".
Esta concatenación permite identificar rápidamente a un jugador en el contexto de una temporada específica, eliminando posibles duplicados y evitando ambigüedades en el análisis.

- **Codificación de la Columna "Pos":** La columna "Pos", que representa la posición en la que juega cada jugador, fue transformada mediante One-Hot Encoding para permitir un procesamiento eficiente en el modelo.
Cuando un jugador aparecía con dos posiciones (por ejemplo, "FW MF" indicando que puede jugar tanto de delantero como de mediocampista), se limitó a solo la primera posición, en este caso "FW" (Forward o Delantero). Esta simplificación fue necesaria para evitar una proliferación de categorías adicionales y reducir el ruido en los datos.

- **Conversión de "Age" a Formato Numérico:** La columna "Age", originalmente en formato texto, fue convertida a un formato numérico para facilitar su uso en el modelo y permitir análisis cuantitativos directos. La edad es un factor que puede influir en el rendimiento de un jugador, por lo que su transformación asegura que se pueda emplear en el cálculo de similitudes y comparaciones.

- **Almacenamiento en Archivos .parquet:** Todas las transformaciones realizadas fueron guardadas en archivos .parquet para preservar el progreso y optimizar el manejo del dataset. El formato .parquet facilita el almacenamiento y procesamiento de grandes volúmenes de datos, lo cual es útil para proyectos de análisis complejos y de larga duración.

Esta conversión permite acceder y cargar los datos en bloques o secciones específicas, optimizando así el uso de memoria y reduciendo los tiempos de carga cuando se requiera realizar pruebas adicionales o ajustar parámetros del modelo.

**Relevancia para el Modelo**

Estas transformaciones fueron esenciales para construir un dataset más compacto y eficiente, con variables estructuradas y normalizadas que facilitan el entrenamiento del modelo de redes neuronales. Además, al almacenar los datos en .parquet, se asegura una experiencia de trabajo más fluida y controlada.
"""

import pandas as pd

# Lista de archivos Parquet de cada temporada
seasons = ["2023-2024", "2022-2023", "2021-2022", "2020-2021", "2019-2020", "2018-2019"]

# Lista para almacenar los DataFrames procesados
processed_dfs = []

# Procesar cada temporada en un ciclo for
for i, season in enumerate(seasons):
    # Leer el archivo Parquet
    df_temp = pd.read_parquet(f"{season}_data.parquet")

    # Eliminar la columna 'Squad' si existe
    if 'Squad' in df_temp.columns:
        df_temp = df_temp.drop(columns=['Squad'])

    # Concatenar temporada y nombre del jugador
    df_temp['Player'] = season + ', ' + df_temp['Player']

    # Tomar solo la primera posición
    df_temp['Pos'] = df_temp['Pos'].apply(lambda x: x.split(',')[0] if pd.notnull(x) else x)

    # Convertir a columnas dummy para posiciones
    df_temp = pd.get_dummies(df_temp, columns=['Pos'], prefix='', prefix_sep='')

    # Filtrar solo las posiciones clave (GK, DF, MF, FW)
    df_temp = df_temp[['Player', 'Age', 'standard_Min', 'standard_Gls', 'standard_Ast',
                       'shooting_Sh', 'shooting_SoT', 'passing_Cmp', 'passing_Att',
                       'defense_Tkl', 'defense_TklW', 'possession_Touches',
                       'GK', 'DF', 'MF', 'FW']]

    # Convertir `Age` a numérico
    df_temp['Age'] = pd.to_numeric(df_temp['Age'], errors='coerce')

    # Agregar el DataFrame procesado a la lista
    processed_dfs.append(df_temp)

    # Guardar el fragmento procesado en disco
    df_temp.to_parquet(f"processed_{season}.parquet")
    print(f"Fragmento {i + 1}/{len(seasons)} guardado en disco.")

    # Imprimir el progreso de cada fragmento
    print(f"Fragmento {i + 1}/{len(seasons)} procesado: {df_temp.shape[0]} filas")

# Concatenar todos los fragmentos procesados en un único DataFrame
df = pd.concat(processed_dfs, ignore_index=True)
print("Concatenación completa.")

"""###**Revisión Final del DataFrame: Paso Clave para el Análisis de Similitud de Jugadores**

El DataFrame final, tras la concatenación de los archivos .paquet obtenidos de FBREF, presenta una colección masiva de 26,162,994 registros, con un total de 16 columnas que describen atributos clave de cada jugador. Estos incluyen tanto estadísticas ofensivas y defensivas como datos de posesión y categorización de posición, lo cual proporciona una base de datos integral para modelar similitudes entre jugadores de la Premier League.

1. **Estructura del DataFrame**

   - **Tamaño y Dimensiones**: El DataFrame cuenta con casi 26.2 millones de filas, lo que representa una cantidad impresionante de datos que abarca múltiples temporadas y jugadores. El uso de 2.4 GB de memoria refleja la densidad de información y la necesidad de un manejo eficiente.
   - **Columnas y Tipos de Datos**:
      - **Variables numéricas:** Las columnas como `Age`, `standard_Min` (minutos jugados), `standard_Gls` (goles anotados), y `passing_Att` (pases intentados) son de tipo `float64`, indicando que los datos permiten valores continuos.
      - **Variables categóricas booleanas:** Las columnas `GK`, `DF`, `MF`, y `FW` identifican la posición del jugador como portero, defensa, mediocampista o delantero, representadas por valores `True` o `False`.
      - **Memoria**: La carga de memoria es considerable, lo cual sugiere optimizar el proceso si el modelo requiere iteraciones frecuentes sobre el conjunto completo.

2. **Desglose Estadístico de Variables Numéricas**

   El uso de `.describe()` arroja una serie de estadísticas que describen el rango, la distribución y la variabilidad en el rendimiento de los jugadores:
   - **Edad**: Con una media de 25.6 años y un rango entre 15 y 39 años, los datos representan tanto jóvenes talentos como veteranos experimentados de la liga.
   - **Minutos Jugados**: La mediana de 952 minutos muestra que muchos jugadores tienen un tiempo de juego moderado, mientras que el máximo de 3420 minutos sugiere titulares que han jugado una temporada completa.
   - **Goles y Asistencias**: La media de goles (1.67) y asistencias (1.17) indica que el grueso de los jugadores no sobresale particularmente en estas métricas. Sin embargo, el máximo de 36 goles y 20 asistencias refleja a los jugadores élite.
   - **Disparos y Pases**: La media de disparos (15.17) y disparos a puerta (5.05) indica una baja tasa de disparos en general, en contraste con los jugadores de ataque que alcanzan hasta 134 intentos de disparo.
   - **Defensiva y Posesión**: Los datos de tackles y posesión ayudan a categorizar a los jugadores más defensivos; con un máximo de 152 tackles y hasta 3988 toques, destacan jugadores de alta participación en la recuperación del balón y manejo de la posesión.

3. **Encabezado y Primeras Filas del DataFrame**

   Al visualizar las primeras cinco filas de `df`, se observan entradas detalladas que incluyen estadísticas como:
   - **Max Aarons** (23 años), un defensa, con 1237 minutos, 2 disparos, y una notable cantidad de 450 pases completados.
   - **Joshua Acheampong** (17 años), un joven defensa que ha jugado 6 minutos y realizado un pase.
   - **Tyler Adams** (24 años), un mediocampista con 121 minutos y contribuciones en defensa y posesión.

**Conclusión**

La revisión exhaustiva de este DataFrame asegura que el conjunto de datos está listo para su uso en el modelo de redes neuronales que buscará patrones de similitud entre jugadores. Este análisis inicial confirma que los datos abarcan una variedad de métricas relevantes, desde actividad ofensiva y defensiva hasta características de posesión y rol dentro del equipo, proporcionando así una sólida base para el análisis y la clasificación de jugadores de la Premier League en función de sus estilos y rendimientos comparativos.
"""

import pandas as pd

# Lista de archivos Parquet procesados de cada temporada
seasons = ["2023-2024", "2022-2023", "2021-2022", "2020-2021", "2019-2020", "2018-2019"]

# Lista para almacenar los DataFrames procesados
processed_dfs = []

# Leer cada archivo .parquet y agregar a la lista
for season in seasons:
    df_temp = pd.read_parquet(f"processed_{season}.parquet")
    processed_dfs.append(df_temp)

# Concatenar todos los DataFrames procesados en un único DataFrame
df = pd.concat(processed_dfs, ignore_index=True)

print("Concatenación completa. El DataFrame tiene", df.shape[0], "filas y", df.shape[1], "columnas.")

print(df.info())  # Revisar tipos de datos y valores nulos
print(df.describe())  # Resumen estadístico
print(df.head())  # Primeras filas del DataFrame

"""##Análisis Exploratorio de Datos (EDA)
1. Distribución de Variables Numéricas
Es fundamental entender cómo están distribuidas las variables para detectar sesgos, outliers y el rango de valores en cada métrica. Esto se puede lograr mediante histogramas o gráficos de densidad.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Selecciona solo las columnas numéricas, excluyendo las booleanas y 'Player'
numerical_columns = ['Age', 'standard_Min', 'standard_Gls', 'standard_Ast',
                     'shooting_Sh', 'shooting_SoT', 'passing_Cmp', 'passing_Att',
                     'defense_Tkl', 'defense_TklW', 'possession_Touches']

# Histogramas para ver la distribución de cada variable
df[numerical_columns].hist(bins=30, figsize=(15, 10), layout=(3, 4))
plt.suptitle('Distribución de Variables Numéricas')
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

"""La imagen presenta histogramas de varias variables numéricas de jugadores de fútbol, revelando cómo están distribuidos los datos para cada métrica y permitiendo una mejor comprensión de la composición y las características de la base de datos.

1. **Edad (Age)**: La distribución de la edad tiene un sesgo hacia los jugadores jóvenes, con un pico alrededor de los 25 años. La mayoría de los jugadores tienen entre 20 y 30 años, y la presencia de jugadores mayores disminuye drásticamente después de los 35 años, lo cual es esperable en deportes de alta exigencia física.

2. **Minutos Jugados (standard_Min)**: La distribución muestra que una gran cantidad de jugadores tienen pocos minutos, con una caída brusca después de los primeros 500 minutos. Pocos jugadores alcanzan el límite superior cercano a los 3000 minutos, lo cual es comúnmente observado en titulares constantes durante una temporada completa.

3. **Goles (standard_Gls)**: Los goles muestran una distribución con un sesgo positivo significativo; la mayoría de los jugadores no anotan o anotan muy pocos goles en la temporada. Solo unos pocos logran cifras altas, lo que indica a jugadores con un rol específico de goleador.

4. **Asistencias (standard_Ast)**: Al igual que los goles, las asistencias son escasas para la mayoría de los jugadores, con un alto sesgo positivo. Esto también refleja que las contribuciones directas a los goles son logradas por un grupo reducido de jugadores clave en el ataque.

5. **Disparos (shooting_Sh)**: La mayoría de los jugadores tienen menos de 10 disparos, mientras que pocos jugadores exceden los 50 disparos en la temporada. Los jugadores con altos números de disparos son generalmente delanteros o mediocampistas ofensivos.

6. **Disparos a Puerta (shooting_SoT)**: Similar al número total de disparos, los disparos a puerta también están sesgados hacia valores bajos. Solo un pequeño grupo de jugadores logra un número elevado de disparos a puerta, reflejando su capacidad para crear situaciones de peligro.

7. **Pases Completados (passing_Cmp)**: Aunque hay una gran cantidad de jugadores con pocos pases completados, algunos alcanzan cifras significativas (cerca de 3000). Esto sugiere roles diversos en el campo, donde ciertos jugadores son responsables de distribuir el balón constantemente.

8. **Pases Intentados (passing_Att)**: La distribución de pases intentados es similar a la de pases completados, con pocos jugadores alcanzando cifras altas. Este es un indicativo de la participación en el juego, donde jugadores en roles de mediocampo suelen tener cifras elevadas en esta métrica.

9. **Tackles (defense_Tkl)**: La mayoría de los jugadores tienen un número bajo de tackles, mientras que unos pocos superan los 50, indicando jugadores con roles defensivos definidos, como defensas y mediocampistas de contención.

10. **Tackles Ganados (defense_TklW)**: Este gráfico es similar al de tackles totales, con una mayoría de jugadores logrando pocos tackles ganados. Este dato puede ayudar a identificar la eficiencia en el juego defensivo.

11. **Toques (possession_Touches)**: Los toques tienen una distribución con sesgo positivo, donde la mayoría de los jugadores tienen menos de 500 toques, mientras que algunos pocos superan los 3000. Esto sugiere diferentes niveles de participación en la posesión del balón y roles de alta influencia en el juego.

2. Relación entre Variables (Correlación)
El siguiente paso es observar la correlación entre variables. Esto nos permite ver qué estadísticas tienden a moverse juntas, lo que puede ayudar en la selección de características para el modelo y reducir la multicolinealidad.
"""

# Mapa de calor de la matriz de correlación
plt.figure(figsize=(12, 8))
sns.heatmap(df[numerical_columns].corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Matriz de Correlación entre Variables Numéricas")
plt.show()

"""La imagen muestra una matriz de correlación entre variables numéricas, útil para identificar relaciones lineales entre métricas de rendimiento de los jugadores. Las correlaciones cercanas a 1 indican relaciones positivas fuertes, mientras que las cercanas a -1 indican relaciones negativas fuertes.

**standard_Min y possession_Touches (0.83):** Existe una correlación positiva fuerte entre los minutos jugados y los toques de balón. Esto sugiere que los jugadores que están más tiempo en el campo suelen tener más posesión, lo cual es lógico, ya que acumulan más intervenciones.

**passing_Cmp y passing_Att (0.99):** Como se esperaba, los pases completados y los pases intentados tienen una correlación positiva muy fuerte. Esto indica que la cantidad de intentos de pase y pases completados están estrechamente relacionados, generalmente porque los jugadores con muchos intentos suelen ser buenos en pases.

**shooting_SoT y shooting_Sh (0.96):** Existe una fuerte correlación positiva entre los disparos a puerta y el total de disparos, lo cual indica que la mayoría de los jugadores que disparan mucho también tienen un alto número de disparos a puerta, reflejando consistencia en su capacidad para generar oportunidades de gol.

**standard_Gls y shooting_SoT (0.81):** Los goles tienen una correlación positiva alta con los disparos a puerta, lo cual sugiere que los jugadores que realizan más disparos a puerta tienden a marcar más goles.

**passing_Att y possession_Touches (0.94):** La frecuencia de pases intentados y los toques de posesión están fuertemente correlacionados, lo que refleja la participación de un jugador en el flujo de juego. Los jugadores que tocan el balón frecuentemente también suelen intentar muchos pases.

**defense_Tkl y defense_TklW (0.98):** Los tackles y los tackles ganados también tienen una correlación alta, lo cual es lógico, ya que los jugadores que realizan muchos tackles tienden a ganar una buena proporción de ellos.

**Age:** La edad tiene una correlación débil con casi todas las variables (valores cercanos a 0), lo cual sugiere que no hay una relación lineal significativa entre la edad y estas métricas de rendimiento. Esto puede indicar que los atributos de rendimiento están más relacionados con el rol del jugador que con su edad.

3. Análisis de la Edad y Posiciones de los Jugadores
Analizar la distribución de la edad puede darnos pistas sobre la experiencia promedio en cada posición y si ciertas posiciones tienden a tener jugadores de cierta edad.
"""

# Boxplot de la edad por posición
plt.figure(figsize=(10, 6))
sns.boxplot(data=df, x='Age', y=df[['GK', 'DF', 'MF', 'FW']].idxmax(axis=1))
plt.title("Distribución de Edad por Posición")
plt.xlabel("Edad")
plt.ylabel("Posición")
plt.show()

"""- Explicación: Esto nos permite ver si ciertas posiciones, como GK (portero), tienden a tener jugadores de mayor o menor edad. También puede ayudarnos a identificar si existen posiciones específicas con más jugadores jóvenes o experimentados.

4. Análisis de Goles y Asistencias
Ver la relación entre standard_Gls (goles) y standard_Ast (asistencias) puede darnos una idea de los jugadores que no solo marcan goles, sino que también generan oportunidades de gol.
"""

# Gráfico de dispersión entre Goles y Asistencias
plt.figure(figsize=(8, 6))
sns.scatterplot(data=df, x='standard_Gls', y='standard_Ast', alpha=0.5)
plt.title("Relación entre Goles y Asistencias")
plt.xlabel("Goles")
plt.ylabel("Asistencias")
plt.show()

"""- Explicación: Esto puede ayudar a identificar a jugadores versátiles, es decir, aquellos que no solo marcan goles, sino que también realizan asistencias. Además, permite ver outliers, como jugadores con muchos goles o asistencias excepcionales.

5. Análisis de Participación en el Juego
Podemos ver la cantidad de minutos (standard_Min) jugados y los toques de posesión (possession_Touches) para entender la participación de los jugadores en el juego.
"""

# Gráfico de dispersión entre Minutos y Toques de Posesión
plt.figure(figsize=(8, 6))
sns.scatterplot(data=df, x='standard_Min', y='possession_Touches', alpha=0.5)
plt.title("Relación entre Minutos Jugados y Toques de Posesión")
plt.xlabel("Minutos Jugados")
plt.ylabel("Toques de Posesión")
plt.show()

"""- Explicación: Esta gráfica puede ayudarnos a ver si hay jugadores que, aunque juegan pocos minutos, tocan el balón frecuentemente. Puede ser útil para entender la influencia de ciertos jugadores en el juego, ya sea en posiciones defensivas o de ataque.

6. Evaluación de Outliers
Una revisión final de los outliers es útil para evaluar si algunos valores extremos podrían ser atípicos o errores en los datos, que podrían sesgar los resultados del modelo.
"""

# Boxplot de cada variable numérica para identificar outliers
plt.figure(figsize=(15, 10))
df[numerical_columns].boxplot(rot=45)
plt.title("Detección de Outliers en Variables Numéricas")
plt.show()

"""- Explicación: Las variables con outliers extremos pueden requerir transformación o escalado especial, especialmente si estos valores no representan comportamientos normales en el deporte.

## Escalado de Datos

Escalar los datos es un paso esencial en el preprocesamiento para modelos de aprendizaje automático, especialmente en redes neuronales, ya que estos modelos son sensibles a las diferencias de escala entre variables. Si las variables están en rangos muy distintos, el modelo podría interpretar que algunas son más importantes simplemente por su escala, lo que no siempre refleja su verdadera contribución a la predicción. El escalado de datos, por lo tanto, transforma todas las variables para que tengan una escala similar, eliminando este sesgo de magnitud.

**Opciones de Escalado**

Para este tipo de problemas, existen dos métodos de escalado particularmente comunes y efectivos:

- **Estandarización (Standard Scaling)**: Este método normaliza los datos de manera que tengan una media de 0 y una desviación estándar de 1. La estandarización es ideal cuando los datos tienen una distribución aproximadamente normal y se requiere que los valores se ubiquen alrededor de la media en una escala uniforme.

  **Fórmula**:
  \[
  z = \frac{x - \text{media}}{\text{desviación estándar}}
  \]

- **Normalización (Min-Max Scaling)**: Este método ajusta los valores de las variables a un rango específico, típicamente entre 0 y 1 o -1 y 1. La normalización conserva la proporción de los datos originales, lo que es útil para datos con distribuciones heterogéneas. Es especialmente adecuada cuando se desea que las variables estén en un rango definido y consistente.

  **Fórmula**:
  \[
  x_{\text{scaled}} = \frac{x - x_{\text{min}}}{x_{\text{max}} - x_{\text{min}}}
  \]

**Recomendación para Estadísticas de Jugadores**

Dado que estamos trabajando con estadísticas de jugadores, las cuales pueden tener diferentes rangos y distribuciones, la **normalización con Min-Max Scaling** sería una opción adecuada. Este enfoque permite que todas las variables tengan una escala similar, manteniendo la proporción entre ellas. De este modo, la red neuronal podrá identificar patrones de manera más efectiva, sin verse afectada por diferencias de escala entre las métricas.
"""

import pandas as pd

# Lista de archivos Parquet procesados de cada temporada
seasons = ["2023-2024", "2022-2023", "2021-2022", "2020-2021", "2019-2020", "2018-2019"]

# Lista para almacenar los DataFrames procesados
processed_dfs = []

# Leer cada archivo .parquet y agregar a la lista
for season in seasons:
    df_temp = pd.read_parquet(f"processed_{season}.parquet")
    processed_dfs.append(df_temp)

# Concatenar todos los DataFrames procesados en un único DataFrame
df = pd.concat(processed_dfs, ignore_index=True)

print("Concatenación completa. El DataFrame tiene", df.shape[0], "filas y", df.shape[1], "columnas.")

"""En el proceso de preprocesamiento de datos, se observó que los archivos `.parquet` contenían un volumen significativo de filas con valores NaN (valores faltantes). Estos valores NaN ocupaban millones de filas en el DataFrame, inflando el tamaño de los archivos y afectando la eficiencia del análisis y procesamiento de los datos.

Para mejorar la manejabilidad del conjunto de datos y optimizar los recursos de procesamiento, se optó por eliminar estas filas que no contenían información relevante. Este paso permitió reducir drásticamente el tamaño del DataFrame, pasando de más de **26 millones de filas** con valores en su mayoría vacíos a un conjunto mucho más manejable de **aproximadamente 4,800 filas**.

Esta reducción del tamaño del DataFrame no solo facilita la carga y el análisis de los datos, sino que también contribuye a que el modelo de machine learning pueda procesar la información de forma más rápida y eficiente, al reducir el ruido generado por los datos faltantes. Además, un DataFrame más ligero mejora la velocidad de lectura y escritura en disco, así como el rendimiento general en tareas de manipulación y exploración de datos.
"""

# Seleccionar solo las columnas numéricas para el escalado
numerical_columns = ['Age', 'standard_Min', 'standard_Gls', 'standard_Ast',
                     'shooting_Sh', 'shooting_SoT', 'passing_Cmp', 'passing_Att',
                     'defense_Tkl', 'defense_TklW', 'possession_Touches']

# Seleccionar solo las filas que no tienen NaN en ninguna de las columnas
df = df.dropna(subset=numerical_columns).copy()

# Restablecer el índice después de eliminar las filas
df.reset_index(drop=True, inplace=True)

# Confirmar que el DataFrame ahora tiene las filas correctas
print("Tamaño final del DataFrame:", df.shape)
print("Conteo de NaN después de eliminación:\n", df.isna().sum())

"""**Implementación del escalado**

Usaremos MinMaxScaler de Scikit-learn para realizar el escalado en las columnas numéricas.
"""

import numpy as np
from sklearn.preprocessing import MinMaxScaler

# Crear una instancia de MinMaxScaler
scaler = MinMaxScaler()

# Aplicar el escalador solo en las columnas numéricas
df[numerical_columns] = scaler.fit_transform(df[numerical_columns])

"""Con esto, todas las variables numéricas estarán en un rango entre 0 y 1, lo cual es ideal para la red neuronal.

##Preparación del Dataset para el Modelo
Queremos un modelo que sea capaz de identificar jugadores similares a partir de sus estadísticas. Para esto, convertiremos el problema en una búsqueda de similitud usando redes neuronales.

Estructura del Dataset
Vamos a construir un dataset de entrada que incluya todas las características escaladas de los jugadores. Las columnas GK, DF, MF, y FW también serán útiles para permitir que el modelo entienda la posición de cada jugador.

Antes de pasar al modelo, necesitamos:

Dividir el dataset en una matriz de características (X) y, si es necesario, una variable de salida (y). En este caso, la salida del modelo será similaridad, por lo que X será suficiente.
"""

# Seleccionar las columnas que vamos a usar para el modelo
feature_columns = numerical_columns + ['GK', 'DF', 'MF', 'FW']
X = df[feature_columns].values

from sklearn.model_selection import train_test_split

# Dividir en conjuntos de entrenamiento y validación
X_train, X_val = train_test_split(X, test_size=0.2, random_state=42)

X_train = X_train.astype(np.float32)
X_val = X_val.astype(np.float32)
X_df = X_df.astype(np.float32)

X_df = X
X_df = X_df.astype(np.float32)

"""##Entrenamiento del Modelo: Red Neuronal para Búsqueda de Similitud

Para el modelo de similitud de jugadores, con base en los datos disponibles y el objetivo de recomendar jugadores similares, una red neuronal con codificación de características y métricas de distancia es una opción consistente con el objetivo. Se desglosa las partes del modelo propuesto y cómo estas ayudarán a cumplir con el objetivo.

- **Estructura del modelo:**

**Entrada:** Los datos de cada jugador (array X) pasan por una capa densa inicial para aprender una representación interna.

**Capas ocultas:** Se incluyen capas densas con activación ReLU para capturar las complejas relaciones entre características de jugadores.

**Capa de salida:** Una capa densa que representa la "firma" o codificación de cada jugador en un espacio de similitud.

- **Detalles de implementación:**

**Función de activación:** Usaremos ReLU en las capas ocultas para lograr no linealidad y tanh en la capa de salida para que los valores estén en un rango acotado, lo cual es útil al medir distancias.

**Optimización:** El optimizador Adam es recomendado por su eficiencia en ajustes de pesos.

**Métrica de distancia:** Una vez entrenada, se puede calcular la similitud entre jugadores utilizando la distancia euclidiana o el coseno en el espacio de características generado por la capa de salida.

- **Función de pérdida y métrica:**

Dado que el objetivo es la similitud en un espacio de características continuo, usamos Mean Squared Error (MSE) para optimizar la correspondencia de jugadores. Aunque el modelo no esté orientado a clasificación directa, accuracy podría ser opcional como métrica de evaluación.

**Estructura del modelo en Keras:**
"""

import tensorflow as tf
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model

# Obtener el número de características
input_dim = X_train.shape[1]

# Definir el modelo secuencial con capas ajustadas
inputs = Input(shape=(input_dim,))
x = Dense(64, activation='relu')(inputs)
x = Dense(32, activation='relu')(x)
embedding = Dense(16, activation='relu', name="embedding_layer")(x)
x = Dense(32, activation='relu')(embedding)
outputs = Dense(input_dim, activation='linear')(x)

model = Model(inputs=inputs, outputs=outputs)

# Ajustar el optimizador
model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4))
model.summary()

"""- **Explicación de la estructura propuesta:**

**Capas:** La reducción gradual en el número de neuronas ayuda al modelo a aprender representaciones de menor dimensionalidad, efectivas para medir similitud.

**Función de pérdida y optimización:** Adam facilita la convergencia eficiente y mse es adecuada para capturar similitudes.

- **Entrenar el modelo**

Entrenemos el modelo usando el conjunto de datos preparado (X), dividiéndolo en datos de entrenamiento y de validación para evaluar el rendimiento del modelo en datos nuevos y evitar sobreajuste (overfitting).
"""

# Entrenar el modelo
history = model.fit(X_train, X_train, epochs=50, batch_size=32, validation_data=(X_val, X_val))

"""- **Evaluar el Modelo y Calcular Métricas de Rendimiento**

Después del entrenamiento, puedes usar el conjunto de validación para obtener métricas y visualizar el rendimiento:

**Error Cuadrático Medio (MSE):**

Para ver si el modelo está capturando bien las similitudes, visualizaremos la pérdida (loss) y pérdida de validación (val_loss) a lo largo de las épocas de entrenamiento.
"""

mse = model.evaluate(X_val, X_val, verbose=0)
print(f"Mean Squared Error (MSE) on validation data: {mse}")

"""**R² Score:**

Para el R², usa r2_score de sklearn, que mide qué tan bien las predicciones se ajustan a los datos reales.
"""

from sklearn.metrics import r2_score

# Obtener predicciones en el conjunto de validación
y_pred = model.predict(X_val)

# Assuming y_pred has shape (964, 1) and X_val has shape (964, 15)
# Calculate R² for each feature
r2_scores = []
for i in range(X_val.shape[1]):  # Iterate over the 15 features
    r2 = r2_score(X_val[:, i], y_pred[:, i])  # Compare the i-th feature of X_val with y_pred
    r2_scores.append(r2)

# Print or analyze the individual R² scores
print(f"R² Scores for each feature: {r2_scores}")

# Calculate average R² if needed
avg_r2 = np.mean(r2_scores)
print(f"Average R² Score: {avg_r2}")

"""Los valores de \( R^2 \) para cada característica varían de 0.8 a 0.99, lo que significa que el modelo tiene un desempeño sólido en la mayoría de las variables, aunque algunas están mejor explicadas por el modelo que otras."""

import matplotlib.pyplot as plt

# Gráfica de la pérdida en entrenamiento y validación
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Training and Validation Loss Over Epochs')
plt.show()

"""**¿Qué buscamos aquí?**

Una brecha pequeña entre la pérdida de entrenamiento y la de validación indica que el modelo está generalizando bien.
Una pérdida de validación constante o en descenso muestra que el modelo no está sobreajustando.

##**Comparar Jugadores Similares**

**Extracción de Representaciones Latentes**

Para identificar similitudes entre jugadores, necesitamos extraer la representación latente de cada jugador en la capa más pequeña de tu autoencoder (la capa Dense(16) en este caso), que captura las características clave.
"""

from tensorflow.keras.models import Model
# Crear un modelo de embedding para extraer la representación comprimida
embedding_model = Model(inputs=model.input, outputs=model.get_layer("embedding_layer").output)


# Obtener embeddings para todos los jugadores
player_embeddings = embedding_model.predict(X_df)
player_names = df['Player'].values  # Lista de nombres de jugadores

"""**Explicación:**
Este fragmento de código crea un nuevo modelo (`embedding_model`) basado en el modelo ya entrenado, pero con la salida de la capa de 16 unidades (representación latente). Usamos esta salida como un resumen de las características del jugador, donde se retiene la información relevante para el análisis de similitud.

1. `Model(inputs=model.input, outputs=model.get_layer("embedding_layer").output)` define un modelo con la misma entrada que tu autoencoder, pero que entrega la salida de la tercera capa (la de 16 neuronas).
2. `embedding_model.predict(X_df)` aplica este modelo a los datos completos `X_df`, generando las representaciones latentes de cada jugador y almacenándolas en `player_embeddings`. Esta variable contiene vectores que representan a cada jugador en un espacio de características comprimido, ideal para comparaciones de similitud.

**Encontrar Jugadores Similares a un Jugador Dado**

Con estos embeddings, usa la distancia euclidiana o la distancia de coseno para medir la similitud. Ahora se puede implementar la búsqueda de jugadores similares:
"""

from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Define el índice del jugador que deseas buscar (en este ejemplo, el primero en el conjunto de validación)
index = 0  # Cambia a otro número según el jugador que quieras buscar

# Obtén el embedding del jugador seleccionado
player_embedding = player_embeddings[index].reshape(1, -1)

# Calcula la similitud coseno entre el jugador seleccionado y todos los jugadores en `X_val_embeddings`
similarities = cosine_similarity(player_embedding, player_embeddings).flatten()

# Ordena por similitud descendente y obtiene los índices de los más similares
similar_indices = similarities.argsort()[::-1][1:6]  # Excluye el jugador mismo y toma los 5 más similares

# Mostrar los resultados
print("Jugador seleccionado:", df.iloc[index])  # Muestra el jugador correspondiente al índice
print("\nJugadores similares:")
for idx in similar_indices:
    print(df.iloc[idx])

"""- **Evaluar la representación de los jugadores (análisis de similitud)**

Para confirmar que el modelo logró representar correctamente los jugadores, se pueden tomar jugadores de prueba (como Marcus Rashford) y comparar su representación con la de otros jugadores. Este paso también ayudará a comprobar visualmente si el modelo está capturando jugadores similares.

1. Localiza el embedding del jugador Marcus Rashford en la temporada 2022-2023.

2. Calcula la similitud coseno de este embedding con los embeddings de todos los demás jugadores.

3. Ordena los resultados en función de la similitud coseno, en orden descendente, para destacar a los jugadores más cercanos a Rashford.

4. Devuelve los nombres de los cinco jugadores más similares (excluyendo a Rashford), los cuales comparten atributos y desempeños similares.
"""

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Función para encontrar los jugadores más similares
def find_similar_players(player_name, player_embeddings, df, n=5):
    # Buscar el índice del jugador en el DataFrame
    player_idx = df[df['Player'] == player_name].index[0]
    player_vec = player_embeddings[player_idx].reshape(1, -1)

    # Calcular la similitud con todos los demás jugadores
    similarities = cosine_similarity(player_vec, player_embeddings).flatten()

    # Obtener los n jugadores más similares (excluyendo al jugador en sí mismo)
    similar_indices = np.argsort(-similarities)[1:n+1]
    similar_players = df.iloc[similar_indices]['Player'].values
    return similar_players

# Ejemplo para encontrar jugadores similares a Marcus Rashford
similar_players = find_similar_players('2022-2023, Marcus Rashford', player_embeddings, df)
print("Jugadores similares a Marcus Rashford:", similar_players)

"""**Conclusión**

La construcción de este modelo de similitud de jugadores en la Premier League mediante redes neuronales representa un esfuerzo considerable en el ámbito de la ciencia de datos aplicada al fútbol, así como en la interpretación de las métricas deportivas avanzadas. A través del uso de embeddings y cálculos de similitud coseno, el modelo logra identificar patrones de juego y rendimiento de jugadores de una forma novedosa, permitiendo que usuarios curiosos y entusiastas puedan conocer a otros jugadores con características y desempeños cercanos a los de figuras reconocidas.

Este proyecto conlleva aprendizajes importantes. Primero, la recolección y organización de datos deportivos en un entorno en constante cambio fue un desafío clave, resuelto mediante técnicas de web scraping y almacenamiento en bases de datos de alta eficiencia como SQLite y archivos en formato .parquet. La preparación y transformación de los datos, aunque compleja, sentó una base sólida para el modelo, asegurando que cada columna seleccionada aportara valor al análisis final y permitiendo un proceso ágil y preciso al interactuar con un volumen considerable de estadísticas.

Otro aprendizaje significativo fue la aplicación de técnicas de embeddings y su capacidad para representar datos no estructurados en un espacio vectorial comprensible para la máquina. Esta estructura no solo simplifica la comparación de jugadores, sino que también abre la puerta a análisis aún más profundos y personalizados, permitiendo al usuario experimentar con datos de diversas temporadas y explorar otros jugadores similares con facilidad.

Finalmente, este modelo y su implementación en Streamlit con gráficos de Pizza Chart no solo muestran el valor del análisis de datos en deportes, sino que destacan la importancia del esfuerzo invertido en desarrollar herramientas de visualización accesibles y útiles. Los resultados obtenidos reflejan la dedicación puesta en cada etapa: desde la preparación de datos hasta la creación de interfaces, para que aficionados y principiantes en el análisis de datos puedan experimentar el potencial de la ciencia deportiva.

Este modelo es una prueba de que, con las herramientas y técnicas adecuadas, el fútbol puede analizarse desde una perspectiva cuantitativa, revelando detalles que antes solo los expertos podían captar. La lista final de jugadores similares a Rashford demuestra que el esfuerzo invertido en explorar las capacidades de redes neuronales y técnicas avanzadas de similitud ofrece resultados relevantes y satisfactorios, acercando el conocimiento y la emoción de los datos deportivos a quienes tienen la curiosidad y el interés de descubrir más allá de las estadísticas tradicionales.
"""